{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zaid\\anaconda2\\envs\\tensorflow3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, sys, h5py, json, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size of any object\n",
    "def memo_obj(obj):\n",
    "    print(sys.getsizeof(obj)/ 1024**2,\" MB\")\n",
    "\n",
    "def change_datatype(df):\n",
    "    int_cols = list(df.select_dtypes(include=['int']).columns)\n",
    "    for col in int_cols:\n",
    "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "    return df\n",
    "\n",
    "def show_img(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def flip_axis(x, axis):\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', \\\n",
    "                      objective='multiclass', metrics='multi_logloss', # multi_error\\\n",
    "                      feval=None, early_stopping_rounds=20, num_boost_round=3000, \\\n",
    "                      verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 30,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 8,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features)\n",
    "    \n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features)\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=10, \n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid']['multi_logloss'][n_estimators-1])\n",
    "\n",
    "    return bst1, evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"..\\\\..\\\\..\\\\train_data\"\n",
    "file_path = os.path.join(base_path,\"file_labels\")\n",
    "img_path = os.path.join(base_path,\"bin-images\")\n",
    "lgb_path = os.path.join(base_path, \"lgb_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289573, 3)\n",
      "(36197, 3)\n",
      "(36197, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(file_path + '\\\\moderate_train.csv', dtype = {'file_name':str, 'lable':np.int8})\n",
    "df_val = pd.read_csv(file_path + '\\\\moderate_val.csv', dtype = {'file_name':str, 'lable':np.int8})\n",
    "df_test = pd.read_csv(file_path + '\\\\moderate_test.csv', dtype = {'file_name':str, 'lable':np.int8})\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_axis = 2\n",
    "row_axis = 1\n",
    "img_col_axis = col_axis - 1\n",
    "img_row_axis = row_axis - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, batch_size, img_path, num_classes, \\\n",
    "                    horizontal_flip = True, vertical_flip = True):\n",
    "    \"\"\"This generator use a pandas DataFrame to read images (df.tile_name) from disk.\n",
    "    \"\"\"\n",
    "    N = df.shape[0]\n",
    "    while True:\n",
    "        for start in range(0, N, batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            if start + batch_size > N: break\n",
    "            for ind in range(start, start + batch_size):\n",
    "                name = df.loc[ind, 'file_name']\n",
    "                img = image.load_img(img_path+'/' + str(name) + '.jpg', target_size=(224, 224))\n",
    "                img = image.img_to_array(img)\n",
    "                labelname=df.loc[ind, 'label'] \n",
    "                x = preprocess_input(img, data_format='channels_last', mode='caffe')\n",
    "                if horizontal_flip:\n",
    "                    x = flip_axis(x, img_col_axis)\n",
    "                if vertical_flip:\n",
    "                    x = flip_axis(x, img_row_axis)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(labelname) \n",
    "            x_batch = np.array(x_batch, dtype = np.float32)\n",
    "            y_batch = to_categorical(y_batch, num_classes = num_classes)\n",
    "            yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output paths\n",
    "model_output_path = \"..\\\\saved_models\\\\\"\n",
    "plots_output_path = \"..\\\\visualization\\\\\"\n",
    "\n",
    "NUM_CLASSES = 6\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "INIT_LR = 5e-3\n",
    "PATIENCE = 20\n",
    "model_name = \"resnet(caffe)_adam_\"+str(BATCH_SIZE) + \"_random_augment\"\n",
    "base_filepath = model_output_path + model_name + \".hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_model(base_filepath)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('flatten_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_points = [-1,len(df_train)//4,len(df_train)//2,len(df_train)*3//4,len(df_train)]\n",
    "for i in range(len(cut_points)-1):\n",
    "    df_train_samp = df_train.loc[cut_points[i]+1:cut_points[i+1], :]\n",
    "    print(\"Starting:\", cut_points[i]+1, \"to\", cut_points[i+1])\n",
    "    df_train_samp.index = range(len(df_train_samp))\n",
    "    print(\"len(df_train_samp):\", len(df_train_samp))\n",
    "    train_features = model.predict_generator(batch_generator(df_train_samp,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              img_path=img_path,\n",
    "                                              num_classes=NUM_CLASSES,\n",
    "                                              horizontal_flip = False, vertical_flip = False),\n",
    "                                          steps = len(df_train_samp) // BATCH_SIZE,\n",
    "                                          verbose = 1)\n",
    "    train_features = pd.DataFrame(train_features)\n",
    "    train_features['label'] = df_train_samp.loc[:train_features.shape[0], 'label']\n",
    "    train_features['sharpness'] = df_train_samp.loc[:train_features.shape[0], 'sharpness']\n",
    "    train_features['file_name'] = df_train_samp.loc[:train_features.shape[0], 'file_name'].astype(str)\n",
    "    train_features.columns = [\"res_clf_\" + str(i) for i in list(train_features.columns[:2048])] + ['label', 'sharpness', 'file_name']\n",
    "    change_datatype(train_features).to_csv(lgb_path +'\\\\' + model_name + '_train_'+'orig_'+str(i)+'.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = model.predict_generator(batch_generator(df_val, \n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  img_path=img_path,\n",
    "                                                  num_classes=NUM_CLASSES,\n",
    "                                                  horizontal_flip = False, vertical_flip = False),\n",
    "                                              steps = len(df_val) // BATCH_SIZE,\n",
    "                                              verbose = 1)\n",
    "val_features = pd.DataFrame(val_features)\n",
    "val_features['label'] = df_val.loc[:val_features.shape[0], 'label']\n",
    "val_features['file_name'] = df_val.loc[:val_features.shape[0], 'file_name'].astype(str)\n",
    "val_features['sharpness'] = df_val.loc[:val_features.shape[0], 'sharpness']\n",
    "val_features.columns = [str(i) for i in list(train_features.columns[:2048])] + ['label', 'file_name', 'sharpness']\n",
    "\n",
    "test_features = model.predict_generator(batch_generator(df_test, \n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              img_path=img_path,\n",
    "                                              num_classes=NUM_CLASSES,\n",
    "                                              horizontal_flip = False, vertical_flip = False),\n",
    "                                          steps = len(df_test) // BATCH_SIZE,\n",
    "                                          verbose = 1)\n",
    "test_features = pd.DataFrame(test_features)\n",
    "test_features['label'] = df_test.loc[:test_features.shape[0], 'label']\n",
    "test_features['file_name'] = df_test.loc[:test_features.shape[0], 'file_name'].astype(str)\n",
    "test_features['sharpness'] = df_test.loc[:test_features.shape[0], 'sharpness']\n",
    "test_features.columns = [str(i) for i in list(train_features.columns[:2048])] + ['label', 'file_name', 'sharpness']\n",
    "\n",
    "change_datatype(val_features).to_csv(lgb_path +'\\\\' + model_name + '_val'+'.csv', index = None)\n",
    "change_datatype(test_features).to_csv(lgb_path +'\\\\' + model_name + '_test'+'.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test-time augmentation for lgb model\n",
    "test_features = model.predict_generator(batch_generator(df_test, \n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              img_path=img_path,\n",
    "                                              num_classes=NUM_CLASSES,\n",
    "                                              horizontal_flip = True, vertical_flip = False),\n",
    "                                          steps = len(df_test) // BATCH_SIZE,\n",
    "                                          verbose = 1)\n",
    "test_features = pd.DataFrame(test_features)\n",
    "test_features['label'] = df_test.loc[:test_features.shape[0], 'label']\n",
    "test_features['file_name'] = df_test.loc[:test_features.shape[0], 'file_name'].astype(str)\n",
    "test_features['sharpness'] = df_test.loc[:test_features.shape[0], 'sharpness']\n",
    "test_features.columns = [str(i) for i in list(train_features.columns[:2048])] + ['label', 'file_name', 'sharpness']\n",
    "change_datatype(test_features).to_csv(lgb_path +'\\\\' + model_name + '_test_hflip.csv', index = None)\n",
    "\n",
    "test_features = model.predict_generator(batch_generator(df_test, \n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              img_path=img_path,\n",
    "                                              num_classes=NUM_CLASSES,\n",
    "                                              horizontal_flip = False, vertical_flip = True),\n",
    "                                          steps = len(df_test) // BATCH_SIZE,\n",
    "                                          verbose = 1)\n",
    "test_features = pd.DataFrame(test_features)\n",
    "test_features['label'] = df_test.loc[:test_features.shape[0], 'label']\n",
    "test_features['file_name'] = df_test.loc[:test_features.shape[0], 'file_name'].astype(str)\n",
    "test_features['sharpness'] = df_test.loc[:test_features.shape[0], 'sharpness']\n",
    "test_features.columns = [str(i) for i in list(train_features.columns[:2048])] + ['label', 'file_name', 'sharpness']\n",
    "change_datatype(test_features).to_csv(lgb_path +'\\\\' + model_name + '_test_vflip.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
