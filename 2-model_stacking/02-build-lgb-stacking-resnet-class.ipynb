{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zaid\\anaconda2\\envs\\tensorflow3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, sys, h5py, json, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size of any object\n",
    "def memo_obj(obj):\n",
    "    print(sys.getsizeof(obj)/ 1024**2,\" MB\")\n",
    "\n",
    "def change_datatype(df):\n",
    "    int_cols = list(df.select_dtypes(include=['int']).columns)\n",
    "    for col in int_cols:\n",
    "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "    return df\n",
    "\n",
    "def show_img(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def flip_axis(x, axis):\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', \\\n",
    "                      objective='multiclass', metrics='multi_logloss', # multi_error\\\n",
    "                      feval=None, early_stopping_rounds=20, num_boost_round=3000, \\\n",
    "                      verbose_eval=250, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 30,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 8,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features)\n",
    "    \n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features)\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=250, \n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid']['multi_logloss'][n_estimators-1])\n",
    "\n",
    "    return bst1, evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"..\\\\..\\\\..\\\\train_data\"\n",
    "file_path = os.path.join(base_path,\"file_labels\")\n",
    "img_path = os.path.join(base_path,\"bin-images\")\n",
    "lgb_path = os.path.join(base_path, \"lgb_data\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "model_name = \"resnet(caffe)_adam_\"+str(BATCH_SIZE) + \"_random_augment\"\n",
    "dtypes = {'file_name':str, 'label':np.int8, 'sharpness':np.float}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in batch: 0\n",
      "Loading in batch: 1\n",
      "Loading in batch: 2\n",
      "Loading in batch: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(289536, 2051)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in train data\n",
    "for i in range(4):\n",
    "    print(\"Loading in batch:\", i)\n",
    "    if i == 0:\n",
    "        df_train = pd.read_csv(lgb_path +'\\\\' + model_name + '_train_orig_'+str(i)+'.csv', dtype = dtypes)\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, pd.read_csv(lgb_path +'\\\\' + model_name + '_train_orig_'+str(i)+'.csv', dtype = dtypes)])\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36192, 2051)\n",
      "(36192, 2051)\n"
     ]
    }
   ],
   "source": [
    "# loading in val and test data\n",
    "df_val = pd.read_csv(lgb_path +'\\\\' + model_name + '_val.csv', dtype = dtypes)\n",
    "df_test = pd.read_csv(lgb_path +'\\\\' + model_name + '_test.csv', dtype = dtypes)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19772ed3f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sums = [np.sum(df_train[i]) for i in df_train.columns[:2048]]\n",
    "plt.plot(range(len(sums)), sums)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = pd.Series(sums)\n",
    "predictors = ['res_clf_' + str(i) for i in sums[sums > 0].index]\n",
    "predictors += ['sharpness']\n",
    "target = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.015, 0.02, 0.03]\n",
    "num_leaves = [11,13,15,17,19, 21]\n",
    "max_depth = [4,5,6,7]\n",
    "min_child_samples = [10,20,30,40,50,60]\n",
    "subsample = [0.7,0.8,0.9]\n",
    "subsample_freq = [1,1,2,3]\n",
    "colsample_bytree = [0.2, 0.25, 0.3]\n",
    "lambda_l1 = [0.01, 0.001, 0.0001]\n",
    "lambda_l2 = [0.01, 0.001, 0.0001]\n",
    "max_bin = [250, 350, 450, 550]\n",
    "num_class = [6,6]\n",
    "\n",
    "def get_random_params():\n",
    "    params = {\n",
    "        'learning_rate': learning_rate[ np.random.randint(len(learning_rate))],\n",
    "        'num_leaves': num_leaves[np.random.randint(len(num_leaves))],  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': max_depth[np.random.randint(len(max_depth))],  # -1 means no limit\n",
    "        'min_child_samples': min_child_samples[np.random.randint(len(min_child_samples))],  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'subsample': subsample[np.random.randint(len(subsample))],  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': subsample_freq[np.random.randint(len(subsample_freq))],  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': colsample_bytree[np.random.randint(len(colsample_bytree))],  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimusm sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'lambda_l1': lambda_l1[np.random.randint(len(lambda_l1))],\n",
    "        'lambda_l2': lambda_l2[np.random.randint(len(lambda_l2))],\n",
    "        'max_bin' : max_bin[np.random.randint(len(max_bin))],\n",
    "        'num_class': num_class[np.random.randint(len(num_class))]\n",
    "    }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training model 0\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.02657\tvalid's multi_logloss: 1.09824\n",
      "[500]\ttrain's multi_logloss: 0.961035\tvalid's multi_logloss: 1.05454\n",
      "[750]\ttrain's multi_logloss: 0.946659\tvalid's multi_logloss: 1.05069\n",
      "[1000]\ttrain's multi_logloss: 0.939866\tvalid's multi_logloss: 1.05013\n",
      "Early stopping, best iteration is:\n",
      "[1039]\ttrain's multi_logloss: 0.939037\tvalid's multi_logloss: 1.05011\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1039\n",
      "multi_logloss: 1.0501058900837434\n",
      "Time taken:  2.3639619827270506  min\n",
      "Test acc: 0.538986516357206\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 1\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.04339\tvalid's multi_logloss: 1.11302\n",
      "[500]\ttrain's multi_logloss: 0.965431\tvalid's multi_logloss: 1.05861\n",
      "[750]\ttrain's multi_logloss: 0.946542\tvalid's multi_logloss: 1.05194\n",
      "[1000]\ttrain's multi_logloss: 0.937547\tvalid's multi_logloss: 1.05085\n",
      "Early stopping, best iteration is:\n",
      "[1013]\ttrain's multi_logloss: 0.937168\tvalid's multi_logloss: 1.0508\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1013\n",
      "multi_logloss: 1.0508037582798873\n",
      "Time taken:  2.5733753283818563  min\n",
      "Test acc: 0.537632625994695\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 2\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.984715\tvalid's multi_logloss: 1.07213\n",
      "[500]\ttrain's multi_logloss: 0.944614\tvalid's multi_logloss: 1.05193\n",
      "Early stopping, best iteration is:\n",
      "[705]\ttrain's multi_logloss: 0.93477\tvalid's multi_logloss: 1.05059\n",
      "\n",
      "Model Report\n",
      "n_estimators :  705\n",
      "multi_logloss: 1.0505932452826108\n",
      "Time taken:  2.0360663612683614  min\n",
      "Test acc: 0.5388759946949602\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 3\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.00989\tvalid's multi_logloss: 1.08527\n",
      "[500]\ttrain's multi_logloss: 0.956198\tvalid's multi_logloss: 1.05312\n",
      "[750]\ttrain's multi_logloss: 0.943573\tvalid's multi_logloss: 1.05132\n",
      "[1000]\ttrain's multi_logloss: 0.936661\tvalid's multi_logloss: 1.05064\n",
      "Early stopping, best iteration is:\n",
      "[1012]\ttrain's multi_logloss: 0.936381\tvalid's multi_logloss: 1.05061\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1012\n",
      "multi_logloss: 1.050608650748671\n",
      "Time taken:  3.2293993910153707  min\n",
      "Test acc: 0.5385444297082228\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 4\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.05041\tvalid's multi_logloss: 1.11602\n",
      "[500]\ttrain's multi_logloss: 0.970007\tvalid's multi_logloss: 1.05824\n",
      "[750]\ttrain's multi_logloss: 0.951132\tvalid's multi_logloss: 1.05134\n",
      "[1000]\ttrain's multi_logloss: 0.943051\tvalid's multi_logloss: 1.0506\n",
      "Early stopping, best iteration is:\n",
      "[996]\ttrain's multi_logloss: 0.943144\tvalid's multi_logloss: 1.05059\n",
      "\n",
      "Model Report\n",
      "n_estimators :  996\n",
      "multi_logloss: 1.0505876993523071\n",
      "Time taken:  2.1926766792933146  min\n",
      "Test acc: 0.5374392130857648\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 5\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.989182\tvalid's multi_logloss: 1.07214\n",
      "[500]\ttrain's multi_logloss: 0.949037\tvalid's multi_logloss: 1.0515\n",
      "Early stopping, best iteration is:\n",
      "[715]\ttrain's multi_logloss: 0.940068\tvalid's multi_logloss: 1.05029\n",
      "\n",
      "Model Report\n",
      "n_estimators :  715\n",
      "multi_logloss: 1.0502851039445513\n",
      "Time taken:  1.6422422448794047  min\n",
      "Test acc: 0.5390417771883289\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 6\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.987225\tvalid's multi_logloss: 1.06722\n",
      "[500]\ttrain's multi_logloss: 0.955515\tvalid's multi_logloss: 1.05286\n",
      "[750]\ttrain's multi_logloss: 0.947614\tvalid's multi_logloss: 1.05171\n",
      "[1000]\ttrain's multi_logloss: 0.94339\tvalid's multi_logloss: 1.05111\n",
      "Early stopping, best iteration is:\n",
      "[1004]\ttrain's multi_logloss: 0.943329\tvalid's multi_logloss: 1.05108\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1004\n",
      "multi_logloss: 1.0510776504486228\n",
      "Time taken:  1.535319979985555  min\n",
      "Test acc: 0.5387102122015915\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 7\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.00034\tvalid's multi_logloss: 1.07888\n",
      "[500]\ttrain's multi_logloss: 0.953518\tvalid's multi_logloss: 1.05263\n",
      "[750]\ttrain's multi_logloss: 0.942651\tvalid's multi_logloss: 1.05097\n",
      "[1000]\ttrain's multi_logloss: 0.93656\tvalid's multi_logloss: 1.05045\n",
      "Early stopping, best iteration is:\n",
      "[991]\ttrain's multi_logloss: 0.936729\tvalid's multi_logloss: 1.05042\n",
      "\n",
      "Model Report\n",
      "n_estimators :  991\n",
      "multi_logloss: 1.0504237953921935\n",
      "Time taken:  2.3003817796707153  min\n",
      "Test acc: 0.5382404951370469\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 8\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.04664\tvalid's multi_logloss: 1.11538\n",
      "[500]\ttrain's multi_logloss: 0.967976\tvalid's multi_logloss: 1.05944\n",
      "[750]\ttrain's multi_logloss: 0.94973\tvalid's multi_logloss: 1.0523\n",
      "[1000]\ttrain's multi_logloss: 0.941501\tvalid's multi_logloss: 1.05086\n",
      "Early stopping, best iteration is:\n",
      "[1170]\ttrain's multi_logloss: 0.937765\tvalid's multi_logloss: 1.05062\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1170\n",
      "multi_logloss: 1.0506161634221178\n",
      "Time taken:  2.8856461803118387  min\n",
      "Test acc: 0.5393457117595049\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 9\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.995177\tvalid's multi_logloss: 1.07542\n",
      "[500]\ttrain's multi_logloss: 0.952988\tvalid's multi_logloss: 1.05215\n",
      "Early stopping, best iteration is:\n",
      "[596]\ttrain's multi_logloss: 0.948276\tvalid's multi_logloss: 1.0514\n",
      "\n",
      "Model Report\n",
      "n_estimators :  596\n",
      "multi_logloss: 1.0513958330205986\n",
      "Time taken:  1.1831553300221762  min\n",
      "Test acc: 0.5379641909814323\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 10\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 1.02516\tvalid's multi_logloss: 1.09973\n",
      "[500]\ttrain's multi_logloss: 0.959996\tvalid's multi_logloss: 1.05582\n",
      "[750]\ttrain's multi_logloss: 0.945455\tvalid's multi_logloss: 1.0511\n",
      "[1000]\ttrain's multi_logloss: 0.9386\tvalid's multi_logloss: 1.05035\n",
      "Early stopping, best iteration is:\n",
      "[1062]\ttrain's multi_logloss: 0.9373\tvalid's multi_logloss: 1.05019\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1062\n",
      "multi_logloss: 1.050189431348638\n",
      "Time taken:  2.6441014687220257  min\n",
      "Test acc: 0.5394286030061892\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 11\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.992492\tvalid's multi_logloss: 1.07587\n",
      "[500]\ttrain's multi_logloss: 0.948236\tvalid's multi_logloss: 1.05242\n",
      "[750]\ttrain's multi_logloss: 0.936173\tvalid's multi_logloss: 1.05062\n",
      "Early stopping, best iteration is:\n",
      "[868]\ttrain's multi_logloss: 0.932331\tvalid's multi_logloss: 1.05037\n",
      "\n",
      "Model Report\n",
      "n_estimators :  868\n",
      "multi_logloss: 1.050368978147675\n",
      "Time taken:  1.9082895755767821  min\n",
      "Test acc: 0.5393180813439434\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 12\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.960564\tvalid's multi_logloss: 1.05615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttrain's multi_logloss: 0.939309\tvalid's multi_logloss: 1.05058\n",
      "Early stopping, best iteration is:\n",
      "[480]\ttrain's multi_logloss: 0.9402\tvalid's multi_logloss: 1.05056\n",
      "\n",
      "Model Report\n",
      "n_estimators :  480\n",
      "multi_logloss: 1.0505638992309643\n",
      "Time taken:  1.061608119805654  min\n",
      "Test acc: 0.5390417771883289\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 13\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.989612\tvalid's multi_logloss: 1.07277\n",
      "[500]\ttrain's multi_logloss: 0.94618\tvalid's multi_logloss: 1.05132\n",
      "[750]\ttrain's multi_logloss: 0.934041\tvalid's multi_logloss: 1.0502\n",
      "Early stopping, best iteration is:\n",
      "[783]\ttrain's multi_logloss: 0.932855\tvalid's multi_logloss: 1.05004\n",
      "\n",
      "Model Report\n",
      "n_estimators :  783\n",
      "multi_logloss: 1.0500398033924356\n",
      "Time taken:  2.3197038173675537  min\n",
      "Test acc: 0.5392628205128205\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 14\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[250]\ttrain's multi_logloss: 0.95995\tvalid's multi_logloss: 1.05686\n",
      "[500]\ttrain's multi_logloss: 0.937888\tvalid's multi_logloss: 1.05121\n",
      "Early stopping, best iteration is:\n",
      "[497]\ttrain's multi_logloss: 0.938022\tvalid's multi_logloss: 1.05118\n",
      "\n",
      "Model Report\n",
      "n_estimators :  497\n",
      "multi_logloss: 1.0511768420624443\n",
      "Time taken:  1.2387234767278035  min\n",
      "Test acc: 0.5393180813439434\n",
      "***************************************************************************\n",
      "\n",
      "Starting training model 15\n",
      "preparing validation datasets\n",
      "Training until validation scores don't improve for 30 rounds.\n"
     ]
    }
   ],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "for i in range(300):\n",
    "    start_time = time.time()\n",
    "    params = get_random_params()\n",
    "    \n",
    "    # make sure that num_leaves are less than 2**max_depth\n",
    "    if params['num_leaves'] > 2**params['max_depth']:\n",
    "        params['num_leaves'] = np.random.randint(low = 3, high = 2**params['max_depth'])\n",
    "    \n",
    "    for key,val in params.items(): \n",
    "        res[key].append(val)\n",
    "    objective = 'regression' if params['num_class'] == 1 else 'multiclass'\n",
    "    reg_metrics = ['l1', 'l2'] # Randomly pick l1 for l2\n",
    "    metrics = reg_metrics[np.random.randint(len(reg_metrics))] if params['num_class'] == 1 else 'multi_logloss'\n",
    "    \n",
    "    print(\"Starting training model\", i)\n",
    "    bst, evals_results = lgb_modelfit_nocv(params, \n",
    "                            df_train, \n",
    "                            df_val, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            objective = objective, \n",
    "                            metrics = metrics,\n",
    "                            early_stopping_rounds = 30, \n",
    "                            num_boost_round = 1500)\n",
    "    print(\"Time taken: \", (time.time() - start_time) / 60, \" min\")\n",
    "\n",
    "    label_test_pred = bst.predict(df_test[predictors])\n",
    "    label_test = df_test.loc[:len(label_test_pred), 'label']\n",
    "    if params['num_class'] == 1:\n",
    "        label_test_pred = [np.round(i) for i in label_test_pred]\n",
    "    else:\n",
    "        label_test_pred = [np.argmax(label_test_pred[i]) for i in range(label_test_pred.shape[0])]\n",
    "    conf_mat = confusion_matrix(label_test, label_test_pred)\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        acc = conf_mat[i,i] / np.sum(conf_mat[i, :])\n",
    "        res['label_'+str(i)+'_acc'].append(acc)\n",
    "    model_acc = np.sum([conf_mat[i,i] for i in range(conf_mat.shape[0])]) / len(label_test)\n",
    "    n_estimators = bst.best_iteration\n",
    "    loss = evals_results['valid'][metrics][n_estimators-1]\n",
    "    res['test_acc'].append(model_acc)\n",
    "    res['n_estimators'].append(n_estimators)\n",
    "    res['loss'].append(loss)\n",
    "    print(\"Test acc:\", model_acc)\n",
    "    print(\"*\"*75)\n",
    "    print()\n",
    "    \n",
    "res = pd.DataFrame(res)\n",
    "res.to_csv('random_search_w_sharpness.csv')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
