{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zaid\\anaconda2\\envs\\tensorflow3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os, gc, sys, h5py, json, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print size of any object\n",
    "def memo_obj(obj):\n",
    "    print(sys.getsizeof(obj)/ 1024**2,\" MB\")\n",
    "\n",
    "def change_datatype(df):\n",
    "    int_cols = list(df.select_dtypes(include=['int']).columns)\n",
    "    for col in int_cols:\n",
    "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        else:\n",
    "            df[col] = df[col].astype(np.int64)\n",
    "    return df\n",
    "\n",
    "def show_img(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    plt.imshow(img, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def flip_axis(x, axis):\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n",
    "\n",
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', \\\n",
    "                      objective='multiclass', metrics='multi_logloss', # multi_error\\\n",
    "                      feval=None, early_stopping_rounds=20, num_boost_round=3000, \\\n",
    "                      verbose_eval=250, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 30,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 8,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features)\n",
    "    \n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features)\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, \n",
    "                     xgtrain, \n",
    "                     valid_sets=[xgtrain, xgvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=num_boost_round,\n",
    "                     early_stopping_rounds=early_stopping_rounds,\n",
    "                     verbose_eval=250, \n",
    "                     feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid']['multi_logloss'][n_estimators-1])\n",
    "\n",
    "    return bst1, evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"..\\\\..\\\\..\\\\train_data\"\n",
    "file_path = os.path.join(base_path,\"file_labels\")\n",
    "img_path = os.path.join(base_path,\"bin-images\")\n",
    "lgb_path = os.path.join(base_path, \"lgb_data\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "model_name = \"resnet(caffe)_adam_\"+str(BATCH_SIZE) + \"_random_augment\"\n",
    "dtypes = {'file_name':str, 'label':np.int8, 'sharpness':np.float}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in train data\n",
    "for i in range(4):\n",
    "    print(\"Loading in batch:\", i)\n",
    "    if i == 0:\n",
    "        df_train = pd.read_csv(lgb_path +'\\\\' + model_name + '_train_orig_'+str(i)+'.csv', dtype = dtypes)\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, pd.read_csv(lgb_path +'\\\\' + model_name + '_train_orig_'+str(i)+'.csv', dtype = dtypes)])\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in val and test data\n",
    "df_val = pd.read_csv(lgb_path +'\\\\' + model_name + '_val.csv', dtype = dtypes)\n",
    "df_test = pd.read_csv(lgb_path +'\\\\' + model_name + '_test.csv', dtype = dtypes)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = [np.sum(df_train[i]) for i in df_train.columns[:2048]]\n",
    "plt.plot(range(len(sums)), sums)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = pd.Series(sums)\n",
    "predictors = ['res_clf_' + str(i) for i in sums[sums > 0].index]\n",
    "predictors += ['sharpness']\n",
    "target = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.015, 0.02, 0.03]\n",
    "num_leaves = [11,13,15,17,19, 21]\n",
    "max_depth = [4,5,6,7]\n",
    "min_child_samples = [10,20,30,40,50,60]\n",
    "subsample = [0.7,0.8,0.9]\n",
    "subsample_freq = [1,1,2,3]\n",
    "colsample_bytree = [0.2, 0.25, 0.3]\n",
    "lambda_l1 = [0.01, 0.001, 0.0001]\n",
    "lambda_l2 = [0.01, 0.001, 0.0001]\n",
    "max_bin = [250, 350, 450, 550]\n",
    "num_class = [6,6]\n",
    "\n",
    "def get_random_params():\n",
    "    params = {\n",
    "        'learning_rate': learning_rate[ np.random.randint(len(learning_rate))],\n",
    "        'num_leaves': num_leaves[np.random.randint(len(num_leaves))],  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': max_depth[np.random.randint(len(max_depth))],  # -1 means no limit\n",
    "        'min_child_samples': min_child_samples[np.random.randint(len(min_child_samples))],  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'subsample': subsample[np.random.randint(len(subsample))],  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': subsample_freq[np.random.randint(len(subsample_freq))],  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': colsample_bytree[np.random.randint(len(colsample_bytree))],  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 0,  # Minimusm sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'lambda_l1': lambda_l1[np.random.randint(len(lambda_l1))],\n",
    "        'lambda_l2': lambda_l2[np.random.randint(len(lambda_l2))],\n",
    "        'max_bin' : max_bin[np.random.randint(len(max_bin))],\n",
    "        'num_class': num_class[np.random.randint(len(num_class))]\n",
    "    }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "for i in range(300):\n",
    "    start_time = time.time()\n",
    "    params = get_random_params()\n",
    "    \n",
    "    # make sure that num_leaves are less than 2**max_depth\n",
    "    if params['num_leaves'] > 2**params['max_depth']:\n",
    "        params['num_leaves'] = np.random.randint(low = 3, high = 2**params['max_depth'])\n",
    "    \n",
    "    for key,val in params.items(): \n",
    "        res[key].append(val)\n",
    "    objective = 'regression' if params['num_class'] == 1 else 'multiclass'\n",
    "    reg_metrics = ['l1', 'l2'] # Randomly pick l1 for l2\n",
    "    metrics = reg_metrics[np.random.randint(len(reg_metrics))] if params['num_class'] == 1 else 'multi_logloss'\n",
    "    \n",
    "    print(\"Starting training model\", i)\n",
    "    bst, evals_results = lgb_modelfit_nocv(params, \n",
    "                            df_train, \n",
    "                            df_val, \n",
    "                            predictors, \n",
    "                            target, \n",
    "                            objective = objective, \n",
    "                            metrics = metrics,\n",
    "                            early_stopping_rounds = 30, \n",
    "                            num_boost_round = 1500)\n",
    "    print(\"Time taken: \", (time.time() - start_time) / 60, \" min\")\n",
    "\n",
    "    label_test_pred = bst.predict(df_test[predictors])\n",
    "    label_test = df_test.loc[:len(label_test_pred), 'label']\n",
    "    if params['num_class'] == 1:\n",
    "        label_test_pred = [np.round(i) for i in label_test_pred]\n",
    "    else:\n",
    "        label_test_pred = [np.argmax(label_test_pred[i]) for i in range(label_test_pred.shape[0])]\n",
    "    conf_mat = confusion_matrix(label_test, label_test_pred)\n",
    "    for i in range(conf_mat.shape[0]):\n",
    "        acc = conf_mat[i,i] / np.sum(conf_mat[i, :])\n",
    "        res['label_'+str(i)+'_acc'].append(acc)\n",
    "    model_acc = np.sum([conf_mat[i,i] for i in range(conf_mat.shape[0])]) / len(label_test)\n",
    "    n_estimators = bst.best_iteration\n",
    "    loss = evals_results['valid'][metrics][n_estimators-1]\n",
    "    res['test_acc'].append(model_acc)\n",
    "    res['n_estimators'].append(n_estimators)\n",
    "    res['loss'].append(loss)\n",
    "    print(\"Test acc:\", model_acc)\n",
    "    print(\"*\"*75)\n",
    "    print()\n",
    "    \n",
    "res = pd.DataFrame(res)\n",
    "res.to_csv('random_search_w_sharpness.csv')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
